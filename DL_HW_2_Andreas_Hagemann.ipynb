{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python CODE\n",
    "import numpy as np\n",
    "# DO NOT FORGET TO SPECIFY THE SAME SEED\n",
    "np.random.seed (12345)\n",
    "def initialize (input_dim, hidden_dim, output_dim, batchsize):\n",
    "    W1 = np.random.randn(hidden_dim, input_dim) * 0.01\n",
    "    b1 = np.zeros((hidden_dim,))\n",
    "    W2 = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "    b2 = np.zeros((hidden_dim,))\n",
    "    W3 = np.random.randn(output_dim, hidden_dim) * 0.01\n",
    "    b3 = np.zeros((output_dim,))\n",
    "    # list of all network parameters\n",
    "    parameters = [W1, b1, W2, b2, W3, b3 ]\n",
    "    # minibatch of input instances\n",
    "    x = np.random.rand(input_dim, batchsize)\n",
    "    # ground truths\n",
    "    y = np.random.randn(output_dim, batchsize)\n",
    "    return parameters, x, y\n",
    "# initialize parameters, inputs and targets\n",
    "parameters, x, y = initialize (3, 4, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self):\n",
    "        # optimizable weights and bias\n",
    "        self.W1 = []\n",
    "        self.b1 = []\n",
    "        self.W2 = []\n",
    "        self.b2 = []\n",
    "        self.W3 = []\n",
    "        self.b3 = []\n",
    "        \n",
    "        # Results of the forward pass before and after activation\n",
    "        self.layer1BefAct = []\n",
    "        self.layer1 = []\n",
    "        self.layer2BefAct = []\n",
    "        self.layer2 = []\n",
    "        self.layer3BefAct = []\n",
    "        self.layer3 = []\n",
    "        \n",
    "        # Hidden gradiants calculated in backwards pass\n",
    "        self.L3OutputGrad = []\n",
    "        self.L2OutputGrad = []\n",
    "        self.L2WeightedGrad = []\n",
    "        self.L1OutputGrad = []\n",
    "        self.L1WeightedGrad = []\n",
    "        \n",
    "        # gradients of weight and bias calculated in backwards pass\n",
    "        self.w3_gradient = []\n",
    "        self.b3_gradient = []\n",
    "        self.w2_gradient = []\n",
    "        self.b2_gradient = []\n",
    "        self.w1_gradient = []\n",
    "        self.b1_gradient = []\n",
    "        \n",
    "    # Functions:\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    def sigmoidDeriv(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "        \n",
    "    def setParameters(self, param):\n",
    "        self.W1 = param[0]\n",
    "        self.b1 = param[1]\n",
    "        self.W2 = param[2]\n",
    "        self.b2 = param[3]\n",
    "        self.W3 = param[4]\n",
    "        self.b3 = param[5]\n",
    "        \n",
    "    def loss(self, pred, y):\n",
    "        # squared loss function\n",
    "        M = y.shape[1]\n",
    "        return (1. / M) * np.sum( np.sum(.5 * (pred - y)**2, axis = 0))\n",
    "    \n",
    "    def dloss(self, pred,y):\n",
    "        # derivative of the squared loss function\n",
    "        M = y.shape[1]\n",
    "        return(pred - y) / M\n",
    "    \n",
    "    def layerExec(self, input, weight, bias, act):\n",
    "        # forward pass execution of neuron layer\n",
    "        output = np.dot(weight, input) + bias[:,None]\n",
    "        #run sigmoid when needed\n",
    "        if act:\n",
    "            outputAct = self.sigmoid(output)\n",
    "        else:\n",
    "            outputAct = output\n",
    "        return output, outputAct\n",
    "\n",
    "    def forwardPass(self, input, target):\n",
    "        # forward pass of through all layers of the NN\n",
    "        self.layer1BefAct, self.layer1 = self.layerExec(input, self.W1, self.b1, True)\n",
    "        self.layer2BefAct, self.layer2 = self.layerExec(self.layer1, self.W2, self.b2, True)\n",
    "        self.layer3BefAct, self.layer3 = self.layerExec(self.layer2, self.W3, self.b3, False)\n",
    "        return self.loss(self.layer3, target)\n",
    "    \n",
    "    def backwardPass(self, input, target, printResults):\n",
    "        # backwards pass through all layers of the NN\n",
    "        self.L3OutputGrad = self.dloss(self.layer3, target)\n",
    "        self.L2OutputGrad = np.dot(self.W3.T, self.L3OutputGrad)\n",
    "        self.L2WeightedGrad = self.L2OutputGrad * self.sigmoidDeriv(self.layer2BefAct)\n",
    "        self.L1OutputGrad = np.dot(self.W2.T, self.L2WeightedGrad)\n",
    "        self.L1WeightedGrad = self.L1OutputGrad * self.sigmoidDeriv(self.layer1BefAct)\n",
    "        \n",
    "        # calculation of gradients of weights and bias after backwards pass\n",
    "        w3_gradient = np.dot(self.L3OutputGrad, self.layer2.T)\n",
    "        b3_gradient = np.sum(self.L3OutputGrad, axis = 1)\n",
    "        w2_gradient = np.dot(self.L2WeightedGrad, self.layer1.T)\n",
    "        b2_gradient = np.sum(self.L2WeightedGrad, axis = 1)\n",
    "        w1_gradient = np.dot(self.L1WeightedGrad, input.T)\n",
    "        b1_gradient = np.sum(self.L1WeightedGrad, axis = 1)\n",
    "        \n",
    "        # print gradients for Question 4\n",
    "        if printResults:\n",
    "            print(\"w1_gradient:\", w1_gradient, \"\\n\")\n",
    "            print(\"b1_gradient:\", b1_gradient, \"\\n\")\n",
    "            print(\"w2_gradient:\", w2_gradient, \"\\n\")\n",
    "            print(\"b2_gradient:\", b2_gradient, \"\\n\")\n",
    "            print(\"w3_gradient:\", w3_gradient, \"\\n\")\n",
    "            print(\"b3_gradient:\", b3_gradient, \"\\n\")\n",
    "        gradients = [w1_gradient, b1_gradient, \n",
    "                     w2_gradient, b2_gradient, \n",
    "                     w3_gradient, b3_gradient]\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Loss of the forward Pass: 1.0595073989866606 \n",
      "\n",
      "Question 3: Show gradients of loss function for each parameter separately\n",
      "w1_gradient: [[-3.55164801e-07  4.19035853e-07  3.52847230e-06]\n",
      " [ 2.02412923e-06  1.85604158e-06  1.01560709e-06]\n",
      " [ 1.12728984e-05  8.49377219e-06 -3.51794858e-06]\n",
      " [ 6.96166795e-07  6.27452005e-07  2.95038713e-07]] \n",
      "\n",
      "b1_gradient: [6.68064134e-06 3.46671844e-06 1.26685662e-06 1.08561520e-06] \n",
      "\n",
      "w2_gradient: [[ 3.99898284e-06  4.14561394e-06  3.09285858e-06  1.00056924e-05]\n",
      " [-9.83803475e-04 -9.89356662e-04 -9.86819589e-04 -9.82852021e-04]\n",
      " [ 5.26033041e-04  5.29099114e-04  5.26928985e-04  5.30215253e-04]\n",
      " [ 6.09661103e-04  6.13133340e-04  6.11301153e-04  6.10570429e-04]] \n",
      "\n",
      "b2_gradient: [ 6.07123945e-06 -1.96949231e-03  1.05156448e-03  1.22000815e-03] \n",
      "\n",
      "w3_gradient: [[-0.29601562 -0.29315414 -0.29558964 -0.29427093]\n",
      " [-0.00063616 -0.00062495 -0.00062947 -0.00063094]] \n",
      "\n",
      "b3_gradient: [-0.58798803 -0.00125797] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "myNN = NN()\n",
    "myNN.setParameters(parameters)\n",
    "loss = myNN.forwardPass(x,y)\n",
    "print(\"Question 1: Loss of the forward Pass: {}\".format(loss), \"\\n\")\n",
    "print(\"Question 3: Show gradients of loss function for each parameter separately\")\n",
    "gradients = myNN.backwardPass(x, y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize NN using Backpropagation:\n",
      "Loss Start: 3.1989944074154646e-09 \n",
      "\n",
      "Loss after 1 epochs: 3.1984925766169344e-09 \n",
      "\n",
      "Loss after 2 epochs: 3.197990824585568e-09 \n",
      "\n",
      "Loss after 3 epochs: 3.197489151316404e-09 \n",
      "\n",
      "Loss after 10001 epochs: 6.665423669904652e-10 \n",
      "\n",
      "Loss after 20001 epochs: 1.389888114028477e-10 \n",
      "\n",
      "Loss after 30001 epochs: 2.8990455053779295e-11 \n",
      "\n",
      "Loss after 40001 epochs: 6.047647119152267e-12 \n",
      "\n",
      "Loss after 50001 epochs: 1.2616634664726289e-12 \n",
      "\n",
      "Loss after 60001 epochs: 2.63216032832448e-13 \n",
      "\n",
      "Loss after 70001 epochs: 5.4914433469486175e-14 \n",
      "\n",
      "Loss after 80001 epochs: 1.1456794431756107e-14 \n",
      "\n",
      "Loss after 90001 epochs: 2.3902364109319897e-15 \n",
      "\n",
      "Loss after 100000 epochs: 4.987548973714041e-16 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimizing the NN using a stepsize and multiple epochs\n",
    "print(\"Optimize NN using Backpropagation:\")\n",
    "stepsize = 0.1\n",
    "epochs = 100000\n",
    "print(\"Loss Start: {}\".format(loss), \"\\n\")\n",
    "for i in range(epochs):\n",
    "    j = 0\n",
    "    for n in parameters:\n",
    "        parameters[j] = parameters[j] - (gradients[j] * stepsize)\n",
    "        j += 1\n",
    "    myNN.setParameters(parameters)\n",
    "    loss = myNN.forwardPass(x,y)\n",
    "    gradients = myNN.backwardPass(x, y, False)\n",
    "    i += 1\n",
    "    if i % 10000 == 1 or i <= 3:\n",
    "        print(\"Loss after {} epochs: {}\".format(i, loss), \"\\n\")\n",
    "print(\"Loss after {} epochs: {}\".format(epochs, loss), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-3.49288549,  1.09913006, -2.50246173],\n",
       "        [-2.5763079 , -1.97075992,  0.66218477],\n",
       "        [-8.13738639, -0.83435172,  3.07104694],\n",
       "        [-1.89310082, -0.22804272, -2.55404911]]),\n",
       " array([0.99180998, 1.49733687, 1.96028822, 0.07172028]),\n",
       " array([[-1.47091433, -0.21902798,  2.44689452, -0.69588546],\n",
       "        [-2.64648835, -1.6000124 ,  4.61908868, -1.63929583],\n",
       "        [-2.36986603, -1.12458137,  4.17080493, -1.39581991],\n",
       "        [ 1.06935427,  3.10964707, -0.3220495 ,  0.67759025]]),\n",
       " array([-0.38104509,  0.32015126,  0.16711511, -0.92562583]),\n",
       " array([[-0.05401687, -1.38844162, -0.93789822,  3.28331664],\n",
       "        [-2.10544546, -4.51555498, -3.97217755,  2.74506886]]),\n",
       " array([-0.21452285,  3.22139556])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
